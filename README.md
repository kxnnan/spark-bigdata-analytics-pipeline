# ğŸš€ Spark Big Data Analytics Pipeline

## ğŸ“Œ Project Overview

This project demonstrates a scalable Apache Spark-based Big Data Analytics pipeline processing **1M+ synthetic sales records**.

The pipeline includes:

- Batch data processing using PySpark
- Manual schema definition (production-ready ingestion)
- Revenue aggregation and customer analytics
- Window functions with ranking
- Performance benchmarking (cache vs non-cache)
- Partitioned Parquet storage
- Structured Streaming (real-time processing simulation)

This project showcases distributed data processing and performance optimization techniques used in real-world Data Engineering systems.

---

## ğŸ§± Architecture Flow

CSV Data  
â†“  
Spark Ingestion (Manual Schema)  
â†“  
Data Transformation (withColumn, Aggregations)  
â†“  
Window Functions (Customer Ranking per Country)  
â†“  
Partitioned Parquet Storage (Data Lake Style)  
â†“  
Structured Streaming (Micro-batch Processing)

---

## ğŸ”¥ Key Features

### âœ… 1. Manual Schema Definition
- Avoided `inferSchema`
- Production-ready ingestion
- Improved loading performance

### âœ… 2. Large Dataset Processing
- Generated and processed 1M+ records
- Demonstrated distributed computation

### âœ… 3. Revenue Calculation
Created computed column:
TotalPrice = Amount Ã— Quantity

### âœ… 4. Aggregation
- Country-level revenue calculation
- Customer-level revenue aggregation

### âœ… 5. Window Functions
- Ranked customers per country using:
  - `partitionBy`
  - `orderBy`
  - `rank()`

### âœ… 6. Performance Optimization
- Compared execution time:
  - Without cache
  - With cache
- Observed impact of distributed execution and memory caching

### âœ… 7. Partitioned Parquet Storage
Stored processed data using:
.partitionBy("Country")

Improves:
- Query performance
- Data lake organization
- Parallel reads

### âœ… 8. Structured Streaming
Implemented micro-batch streaming using:
readStream â†’ transform â†’ groupBy â†’ writeStream

Demonstrates real-time analytics pipeline behavior.

---

## ğŸ›  Technologies Used

- Apache Spark (PySpark)
- Python
- Pandas
- NumPy
- Matplotlib

---

## âš¡ Performance Concepts Demonstrated

- Lazy Evaluation
- Narrow vs Wide Transformations
- Shuffle Operations
- Hash Aggregation
- Window Execution Plan
- Micro-Batch Streaming
- Caching Strategy

---

## ğŸ“Š Sample Analytical Output

- Revenue per Country
- Top Customers per Country (Ranked)
- Performance benchmarking comparison

---

## â–¶ How to Run

### 1ï¸âƒ£ Install dependencies
pip install -r requirements.txt


### 2ï¸âƒ£ Open the Notebook

Run all cells in:
spark_bigdata_analytics_pipeline.ipynb

---

## ğŸ“Œ Resume Impact Statement

Designed and implemented a scalable Apache Spark Big Data analytics pipeline processing 1M+ records with window functions, structured streaming, partition optimization, and performance benchmarking.

---

## ğŸ‘¨â€ğŸ’» Author

MTech â€“ Big Data Analytics  
Apache Spark Project â€“ Portfolio Demonstration


